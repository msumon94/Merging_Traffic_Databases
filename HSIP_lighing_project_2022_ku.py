# -*- coding: utf-8 -*-
"""spatial_join_crash data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-5bpCmBtCbRbsXNGc_viOkW58F0nu7ZH
"""

import pandas as pd
!pip install geopandas
import geopandas as gpd

import csv

# 1-  Listing points
listings = pd.read_csv("crash_data.csv")

# 2 - convert to Geopandas Geodataframe
gdf_listings = gpd.GeoDataFrame(listings,   geometry=gpd.points_from_xy(listings.KDOT_Longitude, listings.KDOT_Latitude))

#interchange data


interchange = pd.read_csv("lighting_data.csv")

gdf_interchange = gpd.GeoDataFrame(interchange,   geometry=gpd.points_from_xy(interchange.Long, interchange.Lat ))

!pip install pygeos

!pip uninstall rtree

!pip install rtree

import pygeos
gpd.options.use_pygeos = True

sjoined_listings = gpd.sjoin( gdf_interchange, gdf_listings, op="within")
sjoined_listings.head()

!pip install ArcPy



target_features = "C:/data/usa.gdb/states"
join_features = "C:/data/usa.gdb/cities"
out_feature_class = "C:/data/usa.gdb/states_cities"

arcpy.SpatialJoin_analysis(target_features, join_features, out_feature_class)

# Name: SpatialJoin_Example2.py
# Description: Join attributes of cities to states based on spatial relationships.
# Requirements: os module

# Import system modules
import arcpy
import os

# Set local variables
workspace = r"C:\gpqa\mytools\spatialjoin\usa.gdb"
outWorkspace = r"C:\gpqa\mytools\spatialjoin\output.gdb"
 
# Want to join USA cities to states and calculate the mean city population
# for each state
targetFeatures = os.path.join(workspace, "states")
joinFeatures = os.path.join(workspace, "cities")
 
# Output will be the target features, states, with a mean city population field (mcp)
outfc = os.path.join(outWorkspace, "states_mcp2")
 
# Create a new fieldmappings and add the two input feature classes.
fieldmappings = arcpy.FieldMappings()
fieldmappings.addTable(targetFeatures)
fieldmappings.addTable(joinFeatures)
 
# First get the POP1990 fieldmap. POP1990 is a field in the cities feature class.
# The output will have the states with the attributes of the cities. Setting the
# field's merge rule to mean will aggregate the values for all of the cities for
# each state into an average value. The field is also renamed to be more appropriate
# for the output.
pop1990FieldIndex = fieldmappings.findFieldMapIndex("POP1990")
fieldmap = fieldmappings.getFieldMap(pop1990FieldIndex)
 
# Get the output field's properties as a field object
field = fieldmap.outputField
 
# Rename the field and pass the updated field object back into the field map
field.name = "mean_city_pop"
field.aliasName = "mean_city_pop"
fieldmap.outputField = field
 
# Set the merge rule to mean and then replace the old fieldmap in the mappings object
# with the updated one
fieldmap.mergeRule = "mean"
fieldmappings.replaceFieldMap(pop1990FieldIndex, fieldmap)
 
# Delete fields that are no longer applicable, such as city CITY_NAME and CITY_FIPS
# as only the first value will be used by default
x = fieldmappings.findFieldMapIndex("CITY_NAME")
fieldmappings.removeFieldMap(x)
y = fieldmappings.findFieldMapIndex("CITY_FIPS")
fieldmappings.removeFieldMap(y)
 
#Run the Spatial Join tool, using the defaults for the join operation and join type
arcpy.SpatialJoin_analysis(targetFeatures, joinFeatures, outfc, "#", "#", fieldmappings)



"""# Spatial join of traffic crash data"""

# Commented out IPython magic to ensure Python compatibility.

!pip install pygeos
import pygeos as pg

import pandas as pd
!pip install geopandas
import geopandas as gpd
from shapely.geometry import Point, Polygon
# %matplotlib inline

#reading CSV files. Geopandas does not know CSV files

lighting=pd.read_csv('lighting_data.csv')
lighting.head()

lighting.columns  #we need columns name

#create a dataframe for latitude and longitude
#longitude come first

lighting_points= lighting.apply(lambda row: Point(row.Long, row.Lat), axis=1)   #axis=1 means row. if axis=0 , then it will be column
lighting_points.head()

#creating a geodataframe. EPSG:4326  has unit=degree 
lighting_interchange= gpd.GeoDataFrame(lighting, geometry=lighting_points)  #points are dataframe for latitude and longitude

lighting_interchange.crs={'init': 'epsg:4326'}  #this will tell the python that powerplants has latitude and longitude. Otherwise, python can miss the lat & long

lighting_interchange.head()  #this will have all data from df including latitude& longitude, and geometry (points data).

#plot all the values 
lighting_interchange.plot()

#do the same thing for crash data


crashes=pd.read_csv('crash_data.csv')

crashes.columns   #copy column names from here

#rename the latitude and longitud column name 

crashes.rename(columns={"KDOT Latitude":"Lat"}, inplace=True)   #  it won't work without  inplace=True  

crashes.rename(columns={"KDOT Longitude":"Long"}, inplace=True)

crashes.columns   #copy column names from here

#create a dataframe for latitude and longitude
#longitude come first
crash_points= crashes.apply(lambda row: Point(row.Long, row.Lat), axis=1)   #axis=1 means row. if axis=0 , then it will be column

print(crash_points)

#creating a geodataframe. EPSG:4326  has unit=degree 
crash_interchange= gpd.GeoDataFrame(crashes, geometry=crash_points)  #points are dataframe for latitude and longitude

crash_interchange.crs={'init': 'epsg:4326'}  #this will tell the python that powerplants has latitude and longitude. Otherwise, python can miss the lat & long

print(crash_interchange.head())  #this will have all data from df including latitude& longitude, and geometry (points data).


#plot all the values 
crash_interchange.plot()

#new dataset
crash_interchange.head()
lighting_interchange.head()

#check CRS 
crash_interchange.crs

#check crs 
lighting_interchange.crs

#both dataset has epsg:4326.   epsg:3857  unit is in meter

lighting_interchange= lighting_interchange.to_crs(3857)

crash_interchange= crash_interchange.to_crs(3857)

#check crs again 
lighting_interchange.crs

#check CRS 
crash_interchange.crs

#check the datafram if there is any change in geometry 
lighting_interchange.head()

#geometry has changes. so, I believe this will give our nearest data.

#pg.geos_version

#check the versions
#gpd.show_versions()

#!pip uninstall pygeos

#!pip install --upgrade pygeos==0.10.0

#!pip install -- pygeos==0.10.0

#now we can apply spatial join.This will give only nearest points

lighting_with_crashes=gpd.sjoin_nearest(lighting_interchange, crash_interchange, how='left', max_distance=161)
lighting_with_crashes.head()

lighting_with_crashes.shape

lighting_with_crashes.to_csv("C:\\Users\\s885m749\\Downloads\\HSIP\\Cleaned_data.csv")

crash_interchange.buffer(161)

#to check the dataframe type. 

crash_interchange.head()

#Let us create a buffer zone 
lighting_buffer_test= lighting_interchange.buffer(161)

lighting_buffer_test.head()  #look the geometry is polygon

lighting_buffer_test.shape

#how to create a buffer function 
lighting_buffer_test.plot()

#sample  code from GITTER 

geopandas.sjoin(geopandas.GeoDataFrame(geometry=df1.geometry.buffer(100)), df2)

#buffered geodataframe
lighting_buffer=gpd.GeoDataFrame( lighting, geometry= lighting_interchange.buffer(161))

print(lighting_buffer.shape)   #check the shape. We had 26 rows. 
    
lighting_buffer.head()

#check the CRS. CRS should be 3857. 
lighting_buffer.crs

#can I plot this. 
lighting_buffer.plot()

#here is the spatial join 
lighting_joins_crash= gpd.sjoin( lighting_buffer, crash_interchange, op='contains' )

lighting_joins_crash.shape   #column number should be column number of lighting_buffer  +  column number of crash_interchange

lighting_joins_crash

lighting_joins_crash.columns

#is there null values
lighting_joins_crash.isnull()

# NaN values   pd.isna() and pd.isnull()  both are same
lighting_joins_crash.isna()

#total missing values
lighting_joins_crash.isna().sum()

#getting the data in a csv file
lighting_joins_crash.to_csv('lighting_joins_crash.csv')

