# -*- coding: utf-8 -*-
"""HSIP Signing Project_2022_KU.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gtsjI3blO4-TrrnMsDrgQ6dvXrwdIGik
"""

import pandas as pd

crash = pd.read_csv("Cleaned__Crash_Data.csv")

project = pd.read_csv("Cleaned__2015_2016.csv")

#look the shape of the database
crash.shape

#look the shape of the database
project.shape

project.columns

crash.columns

#check the column names
crash.head()

#change the columns name of project data 
project=project.rename(columns={'Route formatted':'On-Road KDOT Name'})

project.head()  #Check the dataframe

project.shape

#now merge the two databases 
merged_data= pd.merge(project, crash, how='left', on='On-Road KDOT Name')

merged_data.shape

merged_data.head()  #lets see the data

merged_data.columns   #lets get the column name

merged_data.isnull().sum()  #lets see if there are null values

#lets see the unique names of route
merged_data['On-Road KDOT Name'].unique

#lets see the unique names of route
merged_data['On-Road KDOT Name'].nunique()

project['On-Road KDOT Name'].nunique()

crash['On-Road KDOT Name'].nunique()

#finding duplicate values 
#duplicateRows = df[df.duplicated(['col1', 'col2'])]
merged_data[merged_data.duplicated(['Accident Key'])]

#finding duplicate values 
#duplicateRows = df[df.duplicated(['col1', 'col2'])]
crash[crash.duplicated(['Accident Key'])]

crash.duplicated(['Accident Key'])

crash.duplicated(['Accident Key']).sum()

#if there are duplicate rows
crash.duplicated()

crash.duplicated().sum()   #it seems there are no duplicate rows, although Accident Key has duplicate values

crash.loc[crash.duplicated(['Accident Key']), :]

#crash[crash.duplicated(['Accident Key'])]   both are same

#drop duplicates
Cleaned_crash=crash.drop_duplicates(['Accident Key'])
Cleaned_crash.shape

#lets check if we  could able to correctly delete duplicates 
908265-283863

#Now download the data
Cleaned_crash.to_csv('Cleaned_crash_pandas.csv')

#now I can merge the dataset
merged_data_2nd= pd.merge(project, Cleaned_crash, how='left', on='On-Road KDOT Name')

merged_data_2nd.shape

merged_data_2nd.columns

merged_data_2nd.duplicated(['Accident Key']).sum()

merged_data.duplicated().sum()

#Lets try to filter data
project.shape

project.columns

project['Year']

print(project['Year'].unique())
print(project['Year'].nunique())

project[project.Year==2015]

Cleaned_crash.shape

Cleaned_crash.columns

Cleaned_crash['Accident Year'].unique()

#change the columns name of Cleaned_crash data 
Cleaned_crash=Cleaned_crash.rename(columns={'Accident Year':'Accident_Year'})

Cleaned_crash.columns

Cleaned_crash.shape

Cleaned_crash.Accident_Year.isna().sum()

#Filtering data with multiple criteria. For checking 
Cleaned_crash[Cleaned_crash.Accident_Year.isin([2021, 2022])]

#Filtering data multiple criteria. Vertical Bar means 'Or'
#  Cleaned_crash[(Cleaned_crash.Accident_Year==2022) |  (Cleaned_crash.Accident_Year==2021)]

#Cleaned_crash.loc[Cleaned_crash.Accident_Year.isin([2022, 2021])]

#   Cleaned_crash[Cleaned_crash.Accident_Year.isin([2021, 2022])]  
#Both are same

#Filtering data single criteria  with tilde (~) symbol 
New_cleaned_crash= Cleaned_crash[~Cleaned_crash.Accident_Year.isin([2020, 2021, 2022])]
New_cleaned_crash

#This new cleaned crash data has no duplicates. Check it 
New_cleaned_crash.duplicated().sum()

#Check if there are any duplicates Accident Key
New_cleaned_crash.duplicated('Accident Key').sum()

#check the shape.  This dataset has no duplicate and crash data of 2020, 2021, 2022 are not taken 
New_cleaned_crash.shape

#Check there are unnecessarry values in any column. So severity is ok. 
New_cleaned_crash['Accident Severity'].unique()

#Check if there are missing values in accident year
New_cleaned_crash['Accident_Year'].isna().sum()

#Check if there are missing values in other columns 
New_cleaned_crash.isna().sum()

#Check the On-Road KDOT Name are in same format
New_cleaned_crash['On-Road KDOT Name'].nunique()

#Now I need to check if On-Road KDOT Name are in same name in both New_Cleaned_crash data project data
print(project['On-Road KDOT Name'].nunique())
project['On-Road KDOT Name'].unique()

#Replace the few values of column  'On-Road KDOT Name'
project['On-Road KDOT Name'].replace(to_replace={'U050B (Dodge City)', 'U050B (Garden City)'}, value={'U050B ', 'U050B'} , inplace=True)

project['On-Road KDOT Name'].unique()

#replacing spaces with underscrore of all column names 
#  project.columns.str.replace(' ', '_')

#check the project data
project.shape

project.head()

#try merging the data. But we need to merge based on condition

"""# Merging two dataset: HSIP Signing KU  July 2022"""

import pandas as pd

#lets read the data first

crash= pd.read_csv('Cleaned_Crash_Data.csv')

project= pd.read_csv('Cleaned__project_2015_2016.csv')

#lets see the shape 
project.shape

project.columns

crash.columns

#change the columns name of project data 
project=project.rename(columns={'Route formatted':'On-Road KDOT Name'})

project.head()

crash.shape

#check missingvalues. We can not work if there are missing values in year and severity
crash.isnull().sum()

#Look There are missing values for On-Road KDOT Name . We need to delete these missing rows.

#In addition, At-Road KDOT Distance has missing values. If we apply merging condition based on At-Road KDOT Distance, then how I can handle it

#check duplicated values
crash.duplicated().sum()

#check duplicated for accident key. We can also delete all duplicates after merging 
crash['Accident Key'].duplicated().sum()

#drop the duplicates
crash.drop_duplicates(['Accident Key'], inplace=True)

#check if there are duplicates
crash['Accident Key'].duplicated().sum()

#Unique crash year
crash['Accident Year'].unique()

#renaming column name
crash=crash.rename(columns={'Accident Year':'Accident_Year'})

#filter data because we dont need data from 2020, 2021, 2022
#Filtering data single criteria  with tilde (~) symbol 

New_crash= crash[~crash.Accident_Year.isin([2020, 2021, 2022])]

New_crash.shape

New_crash.isna().sum()

project.columns

#rename the column name

project= project.rename(columns={'Route formatted':'On-Road KDOT Name'})

project['On-Road KDOT Name'].unique()

#find the missing values of the column.
New_crash['On-Road KDOT Name'].isna().sum()

#Delete the missing values 
New_crash.dropna( subset=['On-Road KDOT Name'], inplace=True)

#check if there are mising values
New_crash['On-Road KDOT Name'].isna().sum()

#check if there is missing values
project['On-Road KDOT Name'].isna().sum()

# Ensure Route is uppercase

#df1["Route"] = df1["Route"].str.upper()
#df2["Route"] = df2["Route"].str.upper()

project['On-Road KDOT Name']= project['On-Road KDOT Name'].str.upper()
New_crash['On-Road KDOT Name']= New_crash['On-Road KDOT Name'].str.upper()

#adding a column named 'Route' if New_crash['On-Road KDOT Name'] contains project['On-Road KDOT Name'].
for route in project['On-Road KDOT Name'].unique():
    New_crash.loc[New_crash['On-Road KDOT Name'].str.contains(route), "Route"] = route

#check the result again. 
print('Unique route in project data=', project['On-Road KDOT Name'].nunique())

print('Unique route in crash data=', New_crash['Route'].nunique())

print('Total missing new column Route in crash data=', New_crash['Route'].isna().sum())

New_crash['Route'].unique()

project['On-Road KDOT Name'].unique()

#check if things are ok. 
New_crash.head()

#see the project data for the last time
project.head()

#now merge the project and New-crash data. Both inner and left join sgows the same effect

crash_join_signing = pd.merge(project, New_crash, how='inner', left_on='On-Road KDOT Name', right_on='Route', suffixes=('_project','_crash') )

#see the column names
print(crash_join_signing.shape)

crash_join_signing.columns

#check duplicates values 
crash_join_signing['Accident Key'].duplicated().sum()

#drop all the duplicates values from column
crash_join_signing.drop_duplicates(subset=['Accident Key'], inplace=True)

crash_join_signing['Accident Key'].duplicated().sum()

#now check the shapes
crash_join_signing.shape

#lets download a copy of the data
crash_join_signing.to_excel('Full_merged_data_2015_2016.xls')

#check the missing values
crash_join_signing.isna().sum()

crash_join_signing['At-Road KDOT Distance']

crash_join_signing.columns

#Filtering data. Because we need to check whether data are within the State mile posts 
Filtered_after_merged_crash=crash_join_signing.query(' `At-Road KDOT Distance` <= `ST Ending MP` or `Starting MP` <=`At-Road KDOT Distance` <=`Ending MP` or `Starting MP.1`<=`At-Road KDOT Distance`<=`Ending MP.1`')
Filtered_after_merged_crash.head()

#check for duplicates and shape
print(Filtered_after_merged_crash.shape)
Filtered_after_merged_crash['Accident Key'].duplicated().sum()

#Download the excel file 
Filtered_after_merged_crash.to_excel('Filtered_after_crash_merged_signing.xls')



"""# Getting crashes within a subarea: HSIP Signing KU July 2022"""

# Commented out IPython magic to ensure Python compatibility.
#install the libraries

!pip install pygeos
import pygeos as pg

import pandas as pd
!pip install geopandas
import geopandas as gpd
from shapely.geometry import Point, Polygon
# %matplotlib inline

#   Make a list of latitudes and longitudes. 
#   Subarea=325, County=Sherman

lat_point_list = [39.566938, 39.569055, 39.134733, 39.129406]
lon_point_list = [-102.046784, -101.390351, -101.390351, -102.045411]

subarea_geom = Polygon(zip(lon_point_list, lat_point_list))
crs = {'init': 'epsg:4326'}
subarea = gpd.GeoDataFrame(index=[0], crs=crs, geometry=[polygon_geom])

#Plot this subarea. While plotting, remember if epsg:4326 is no plot is shown. 

import folium
m = folium.Map([50.854457, 4.377184], zoom_start=5, tiles='cartodbpositron')
folium.GeoJson(subarea).add_to(m)
folium.LatLngPopup().add_to(m)
m

#read the crash data csv file
crash_data= pd.read_csv('Signing_joins_crash.csv')

#check the column names
crash_data.columns

#rename the column names
crash_data= crash_data.rename(columns={'KDOT Latitude':'KDOT_Latitude', 'KDOT Longitude':'KDOT_Longitude'})

#check if column names changed
crash_data.columns

#check for missing values
crash_data.isna().sum()

#delete missing values by columns
crash_data.dropna(axis=0, subset=['KDOT_Latitude', 'KDOT_Longitude'], inplace=True )
crash_data.isna().sum()

#create a dataframe for latitude and longitude
#longitude come first

crash_points= crash_data.apply(lambda row: Point(row.KDOT_Longitude, row.KDOT_Latitude), axis=1)   #axis=1 means row. if axis=0 , then it will be column
crash_points.head()

#creating a geodataframe. EPSG:4326  has unit=degree 
crash_geodataframe= gpd.GeoDataFrame(crash_data, geometry=crash_points)  #crash_points are dataframe for latitude and longitude

crash_geodataframe.crs={'init': 'epsg:4326'}  #this will tell the python that crash_geodataframe has latitude and longitude. Otherwise, python can miss the lat & long

crash_geodataframe.head()  #this will have all data from df including latitude& longitude, and geometry (points data).

#try plotting this. It takes time because of large dataset
crash_geodataframe.plot()

#Plot it.  folium has been installed before
m = folium.Map([50.854457, 4.377184], zoom_start=5, tiles='cartodbpositron')
folium.GeoJson(crash_geodataframe).add_to(m)
folium.LatLngPopup().add_to(m)
m

#its not showing the map. Will solve it later on

#check the crs
crash_geodataframe.crs

#check the crs
subarea.crs

#check both geodataframes blank
subarea.isna().sum()

#check for crash data. We need geometry column. 
crash_geodataframe.isna().sum()

"""#From data school youtube channel"""

#Both subarea and crash_geodataframe has same epsg. Can I now join this two? 

subarea_joins_crash= gpd.sjoin(subarea, crash_geodataframe, op='contains' )

subarea_joins_crash.shape

#I need to check the sub area info
import pandas as pd

data=pd.read_csv('For_checking_sub_area.csv')

data

data.columns

data.count()

data.groupby('Sub Area').Accident_Year.agg(['count', 'min', 'max', 'mean'])

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
data.groupby('Sub Area').Accident_Year.agg(['count', 'min', 'max', 'mean']).plot(kind='bar')



"""#Plotting polygon with set of latitude and longitude: For check

"""

# Commented out IPython magic to ensure Python compatibility.
#install the libraries

!pip install pygeos
import pygeos as pg

import pandas as pd
!pip install geopandas
import geopandas as gpd
from shapely.geometry import Point, Polygon
# %matplotlib inline

#import geopandas as gpd
#from shapely.geometry import Polygon

lat_point_list = [50.854457, 52.518172, 50.072651, 48.853033, 50.854457]
lon_point_list = [4.377184, 13.407759, 14.435935, 2.349553, 4.377184]

polygon_geom = Polygon(zip(lon_point_list, lat_point_list))
crs = {'init': 'epsg:4326'}
polygon = gpd.GeoDataFrame(index=[0], crs=crs, geometry=[polygon_geom])       

polygon.to_file(filename='polygon.geojson', driver='GeoJSON')
polygon.to_file(filename='polygon.shp', driver="ESRI Shapefile")

polygon

import folium
m = folium.Map([50.854457, 4.377184], zoom_start=5, tiles='cartodbpositron')
folium.GeoJson(polygon).add_to(m)
folium.LatLngPopup().add_to(m)
m

#polygon CSV files
#import line vertex from csv file

#import requires packages
!pip install fiona
import fiona
import pandas as pd

#read the file with polygon data. Look how polygon data are formatted
polyDf = pd.read_csv('polygon_csv.csv', header=0)
polyDf.head()

# define schema
schema = {'geometry':'Polygon', 'properties':[('Name','str')]}
schema

#open a fiona object
polyShp = fiona.open('../ShapeOut/cropPoly.shp', mode='w', driver='ESRI Shapefile', schema = schema, crs = "EPSG:4326")





"""# Different ways of filtering: For check"""

df1 = pd.DataFrame({'Route': ['I050', 'K002', 'U050', 'K224'],
                    'Currency': ['EUR', 'EUR', 'EUR', 'USD'],
                    'KDOT_Distance':[10, 12, 15, 20],
                    'ST_Starting_MP': [0, 0, 0, 0],
                    'ST_Ending_MP': [6, 14, 12, 25],
                    'Starting_Mp':[0, 0, 14, 27],
                    'Ending_Mp':[0, 0, 17, 27]})

df1

#filtering with loops. 
booleans=[]
for length in df1.KDOT_Distance:
  if length <= 14:   
    booleans.append(True)

  

  else:
    booleans.append(False)  

print(booleans[0:5])    

is_long= pd.Series(booleans)
df1[is_long]  #only True values will be shown

#efficient way for filtering 
df1[df1.KDOT_Distance<=14]

df1

df1[df1.KDOT_Distance<=df1.ST_Ending_MP]

# '&' means 'and'. multiple conditions
df1[(df1.KDOT_Distance<=df1.ST_Ending_MP) & (df1.KDOT_Distance<=df1.Ending_Mp)]

# '~' this symbols means opposite. To add this symbol a bracket is used. 
df1[~((df1.KDOT_Distance<=df1.ST_Ending_MP) & (df1.KDOT_Distance<=df1.Ending_Mp))]

df1[(df1.KDOT_Distance<=df1.ST_Ending_MP) | (df1.KDOT_Distance<=df1.Ending_Mp)]

#the df.query takes 'and'/'or' as an operator. But other filtering criteria takes '&'/ '|' as an operator.  
df1.query('KDOT_Distance <= ST_Ending_MP | KDOT_Distance<=Ending_Mp')

df1.query('KDOT_Distance <= ST_Ending_MP | KDOT_Distance<=Ending_Mp')

df1

#multiple criteria. 
df1.query( 'KDOT_Distance<=ST_Ending_MP or Starting_Mp < KDOT_Distance < Ending_Mp')



"""# Different ways of Merging data: For check"""

df1 = pd.DataFrame({'Route': ['I050', 'K002', 'U050', 'K224'],
                    'Currency': ['EUR', 'EUR', 'EUR', 'USD']})
df2 = pd.DataFrame({'Route': ['I050', 'K002', 'U050B', 'I050B', 'K002', 'Old U050', 'K224', 'K224'],
                    
                    'Amount': ['150', '175', '160', '180','150', '175', '160', '180' ],
                    'Comment': ['bla', 'bla', 'bla', 'bla', 'bla', 'bla', 'bla', 'bla']})

# Ensure Route is uppercase
df1["Route"] = df1["Route"].str.upper()
df2["Route"] = df2["Route"].str.upper()


#now match  the df2['route'] with df1['Route'] and overwrite 
for route in df1["Route"].unique():
    df2.loc[df2["Route"].str.contains(route), "Route"] = route
print(df2)

#Following solutions are not working 

#For test  from stack overflow 
df1 = pd.DataFrame({'Invoice': ['20561', '20562', '20563', '20564'],
                    'Currency': ['EUR', 'EUR', 'EUR', 'USD']})
df2 = pd.DataFrame({'Ref': ['20561', 'INV20562', 'INV20563BG', '20564'],
                    'Type': ['01', '03', '04', '02'],
                    'Amount': ['150', '175', '160', '180'],
                    'Comment': ['bla', 'bla', 'bla', 'bla']})



df4 = df1.copy()
for i, row in df1.iterrows():
    tmp = df2[df2['Ref'].str.contains(row['Invoice'])]
    df4.loc[i, 'Amount'] = tmp['Amount'].values[0]

print(df4)    

print('.......2nd Solution...Break.............')

# For check. Second solution
df4 = df2.copy()
df4['Invoice'] = [val for idx, val in enumerate(df1['Invoice']) if val in df2['Ref'][idx]]  #adding an extra column. print df4 to see details 
df_m4 = pd.merge(df1, df4[['Amount', 'Invoice']], how='inner', on='Invoice')
df_m5=pd.merge(df1, df4, how='inner', on='Invoice')

print(df_m4)
print('...same as df_m4................')
print(df_m5)

print('...........3rd Solution.....break..............')

import re
df5 = df2.copy()
df5['Invoice'] = [re.findall(r'(\d{5})', s)[0] for s in df2['Ref']]
df_m5 = df1.merge(df5[['Amount', 'Invoice']], on='Invoice')

print(df_m5)


print('........4th Solution..........Break...........')

df4 = df1.copy()

def get_amount(x):
    return df2.loc[df2['Ref'].str.contains(x), 'Amount'].iloc[0]

df4['Amount'] = df4['Invoice'].apply(get_amount)

print(df4)

print('..............5th Solution..........Break...............')

df4= df2.merge(df1, left_on=df2['Ref'].str.extract(r'(\d+)', expand=False), right_on='Invoice', how='left')
print(df4)

#These two formula I can apply to merge the data  
# and then apply if condition to filter data based on distance

compList = '|'.join(df1['Invoice'].tolist())

df2['compMatch'] = df2.Ref.str.contains(compList)

# drop unmatched articles
df2 = df2[df2['compMatch']==True]



df2['content'] = df2['Ref'].str.lower().str.split()
#df2['matchedName'] = df2['content'].apply(lambda x: [x for x in   df1['Invoice'].tolist() if x in df2['Ref']])


df2['matchedName'] = df2['content'].apply(lambda x: [item for item in x if item in df1['Invoice'].tolist()])

#df2['matchedName'] = df2['content'].apply(lambda x: [x for x in  df2['Ref']  if x in df1['Invoice'].tolist()])


df2